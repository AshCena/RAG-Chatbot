{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN0V3poKsDsR",
        "outputId": "dd3b6d1b-0957-49f4-d1f2-a9bb764044d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "d73y19GrvwnS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXxAhDrXvzqg",
        "outputId": "9a6b5f3d-4443-4bb4-c646-a7c8522df315"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "file_path = '/content/drive/My Drive/dataset.json'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    chit_chat_data = json.load(file)\n",
        "\n",
        "# Load and preprocess novel dataset\n",
        "chit_chat_texts = []\n",
        "for key, value in chit_chat_data.items():\n",
        "    messages = value.get(\"messages\", [])\n",
        "    for message_list in messages:\n",
        "        # print(message_list)\n",
        "        for message in message_list:\n",
        "            text = message.get(\"text\", \"\")\n",
        "            # print(text)\n",
        "            chit_chat_texts.append(text)\n",
        "\n",
        "for i in range(len(chit_chat_texts)):\n",
        "    chit_chat_texts[i] = preprocess_text(chit_chat_texts[i])\n",
        "\n",
        "novel_texts = []\n",
        "for i in range(1, 11):\n",
        "    with open(f'novel_1 ({i}).txt', 'r', encoding='utf-8') as file:\n",
        "        novel_text = file.read()\n",
        "        # Split novel text into paragraphs (you may need to adjust this based on the actual structure)\n",
        "        paragraphs = novel_text.split('.')  # Adjust the delimiter based on your text structure\n",
        "        novel_texts.extend(paragraphs)\n",
        "\n",
        "for i in range(len(novel_texts)):\n",
        "    novel_texts[i] = preprocess_text(novel_texts[i])\n",
        "\n",
        "# Create labels for the datasets (1 for chit-chat, 0 for novel)\n",
        "\n",
        "labels_chit_chat = [1] * len(chit_chat_texts)\n",
        "labels_novel = [0] * len(novel_texts)\n",
        "print('labels_chit_chat : ', len(labels_chit_chat))\n",
        "print('labels_novel : ', len(labels_novel))\n",
        "# Combine datasets\n",
        "all_texts = chit_chat_texts + novel_texts\n",
        "all_labels = labels_chit_chat + labels_novel\n",
        "\n",
        "print('Split the data into training and testing sets')\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_texts, all_labels, test_size=0.001, random_state=42)\n",
        "\n",
        "print('Convert text data to TF-IDF features')\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print('Train a Support Vector Machine (SVM) classifier')\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print('# Make predictions on the test set')\n",
        "y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "print('Evaluate the model')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww5IzsfWsE87",
        "outputId": "f1f00a02-900b-4b64-9337-e2352f62f61a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "labels_chit_chat :  258145\n",
            "labels_novel :  18718\n",
            "Split the data into training and testing sets\n",
            "Convert text data to TF-IDF features\n",
            "Train a Support Vector Machine (SVM) classifier\n",
            "# Make predictions on the test set\n",
            "Evaluate the model\n",
            "Accuracy: 0.97\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.65      0.76        17\n",
            "           1       0.98      1.00      0.99       260\n",
            "\n",
            "    accuracy                           0.97       277\n",
            "   macro avg       0.95      0.82      0.87       277\n",
            "weighted avg       0.97      0.97      0.97       277\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_texts = [\"did you eat yesterday\", \"how are you doing\"]\n",
        "# new_texts[0] = preprocess_text(new_texts[0])\n",
        "new_texts_tfidf = tfidf_vectorizer.transform(new_texts)\n",
        "\n",
        "predictions = svm_classifier.predict(new_texts_tfidf)\n",
        "\n",
        "# Display the predictions\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p3jfl3V2pXW",
        "outputId": "427daecb-ecae-43e9-faed-af698bc95001"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib  # For scikit-learn versions < 0.24\n",
        "\n",
        "# Save the trained model to a pickle file\n",
        "model_filename = '/content/drive/My Drive/svm_model_new.pkl'\n",
        "joblib.dump(svm_classifier, model_filename)\n",
        "joblib.dump(tfidf_vectorizer, '/content/drive/My Drive/tfidf_vectorizer.pkl')\n",
        "print(f\"Trained model saved to {model_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pkal52tDzvJ",
        "outputId": "470c499e-6ea1-4cf2-a10a-7ab92596ca4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model saved to /content/drive/My Drive/svm_model_new.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkcpVtZGHzvL",
        "outputId": "51dc84f8-1ee0-40c3-db00-f7b5d037b2e1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}