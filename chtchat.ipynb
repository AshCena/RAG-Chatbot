{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4551b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b00d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "085fd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "585e2646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dashu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4884447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(chatting_data):\n",
    "    processed_data = []\n",
    "    #print(\"conve\", conversational_data)\n",
    "\n",
    "    for chat_no, chat_data in chatting_data.items():\n",
    "        #print(\"chat_no\", chat_no)\n",
    "        #print(\"chat_data\", chat_data)\n",
    "        messages = []\n",
    "        for message_group in chat_data[\"messages\"]:\n",
    "            \n",
    "            combined_message = \" \".join([message[\"text\"] for message in message_group])\n",
    "            messages.append(combined_message)\n",
    "\n",
    "        \n",
    "        chat_text = \" \".join(messages)\n",
    "        processed_data.append(chat_text)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbdb824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_output_pairs(tokenized_data, window_size=5):\n",
    "    input_output_pairs = []\n",
    "\n",
    "    for chat_tokens in tokenized_data:\n",
    "        for i in range(len(chat_tokens) - window_size):\n",
    "            input_sequence = chat_tokens[i:i + window_size]\n",
    "            #print(\"input_sequence\", input_sequence)\n",
    "            output_sequence = chat_tokens[i + window_size]\n",
    "            #print(\"output_sequence\", output_sequence)\n",
    "\n",
    "            input_output_pairs.append((input_sequence, output_sequence))\n",
    "\n",
    "    return input_output_pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae974c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0a8358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'How', 'are', 'you', 'doing', 'today', '?', 'whats', 'up', 'MD', 'im', 'doing', 'good', 'how', 'are', 'you', 'doing', '?', 'Im', 'alright', ',', 'I', 'just', 'took', 'a', 'nap', '.', 'But', 'it', 'was', 'one', 'of', 'those', 'naps', 'that', 'doesnt', 'help', 'anything', '.', 'It', 'just', 'makes', 'everything', 'worse', 'and', 'you', 'question', 'all', 'your', 'life', 'choices', 'oh', 'wow', 'haha', 'so', 'you', 'still', 'feel', 'tired', 'huh', '?', 'Yeah', 'did', 'you', 'go', 'to', 'bed', 'late', '?', 'I', 'have', 'more', 'of', 'a', 'head', 'ache', 'than', 'anything', 'else', 'dude', 'thats', 'terrible', 'No', ',', 'Its', 'just', 'adjusting', 'to', 'a', 'time', 'change', 'But', 'enough', 'about', 'me', 'What', 'are', 'you', 'up', 'to', '?', 'im', 'refining', 'my', 'skills', 'on', 'Sketch', 'App', 'have', 'you', 'heard', 'of', 'Sketch', '?', 'I', 'havent', 'What', 'is', 'it', '?', 'you', 'know', 'what', 'adobe', 'illustrator', 'is', 'right', '?', 'Yes', 'its', 'a', 'vector', 'graphic', 'editor', 'like', 'illustrator', 'but', 'its', 'a', 'lot', 'more', 'light', 'weight', 'and', 'its', 'for', 'UX', 'designers', 'to', 'design', 'UI', 'Oh', 'nice', '!', 'What', 'have', 'you', 'been', 'practicing', 'designing', 'Ive', 'been', 'trying', 'to', 'learn', 'more', 'tools', 'on', 'sketch', 'and', 'just', 'get', 'faster', 'at', 'it', ']', 'Im', 'doing', 'a', 'free', 'lance', 'job', 'for', 'a', 'professor', 'for', 'his', 'app', 'idea', 'and', 'he', 'needs', 'a', 'mockup', 'of', 'what', 'the', 'app', 'will', 'look', 'like', 'for', 'his', 'pitch', 'Wow', 'that', 'is', 'cool', '!', 'When', 'is', 'that', 'all', 'due', 'by', '?', 'this', 'weekend', 'haha', 'Do', 'you', 'think', 'you', 'will', 'finish', 'or', 'is', 'that', 'a', 'pushy', 'deadline', '?', 'its', 'a', 'deadline', 'I', 'set', 'haha', 'but', 'I', 'think', 'Ill', 'be', 'able', 'to', 'finish', 'I', 'hope', '...', 'if', 'not', 'I', 'can', 'push', 'it', 'back', 'a', 'little', 'Well', 'I', 'wish', 'you', 'the', 'best', 'of', 'luck', 'with', 'that', 'deadline', 'thanks', 'do', 'you', 'have', 'any', 'personal', 'projects', 'you', 'are', 'working', 'on', \"''\", '?', 'Not', 'really', 'projects', '.', 'I', 'finished', 'editing', 'a', 'bunch', 'of', 'engagement', 'pictures', '.', 'So', 'now', 'all', 'I', 'am', 'really', 'doing', 'is', 'just', 'practicing', 'some', 'sketching', 'I', 'can', 'always', 'practice', 'and', 'improve', 'that', 'skill', 'oh', 'coooool', '!', 'was', 'this', 'a', 'paid', 'photoshoot', '?', 'Yep', ',', 'I', 'had', 'a', 'few', 'that', 'I', 'did', 'before', 'I', 'went', 'home', 'for', 'the', 'summer', 'engagement', 'shoots', 'or', 'weddings', 'are', 'goooood', 'money', 'but', 'i', 'hear', 'in', 'utah', 'its', 'super', 'competitive', 'Yeah', 'there', 'are', 'so', 'many', ',', 'every', 'other', 'person', 'takes', 'pictures', 'Its', 'easy', 'to', 'do', 'it', 'for', 'friends', 'and', 'people', 'you', 'know', 'but', 'past', 'that', 'it', 'is', 'hard', 'to', 'get', 'more', 'business', 'yeah', 'true', 'do', 'you', 'think', 'if', 'you', 'wanted', 'to', 'pursue', 'your', 'photography', 'you', 'wuold', 'have', 'to', 'move', 'out', 'of', 'utah', '?', 'Um', 'maybe', ',', 'People', 'need', 'pictures', 'everywhere', 'But', 'I', 'dont', 'really', 'want', 'to', 'do', 'that', ',', 'its', 'fine', 'for', 'the', 'side', ',', 'but', 'too', 'hard', 'to', 'make', 'a', 'living', 'on', 'Remind', 'me', 'what', 'you', 'want', 'to', 'do', 'career', 'wise', '?', 'I', 'want', 'to', 'become', 'a', 'UX', 'desiner', 'if', 'you', 'werent', 'Christian', 'what', 'religion', 'would', 'you', 'be', 'in', 'if', 'at', 'all', '?', 'Hm', 'Maybe', 'Buddhist', 'What', 'about', 'you', '?', 'I', 'think', 'its', 'a', 'really', 'peaceful', 'practice', 'and', 'it', 'would', 'match', 'my', 'personality', 'I', 'probably', 'would', 'be', 'buddhist', 'culturally', 'because', 'IM', 'korean', 'but', 'if', 'i', 'had', 'a', 'choice', 'I', 'would', \"n't\", 'label', 'myself', 'as', 'any', 'religion', 'egnostic', '?', 'is', 'that', 'the', 'term', '?', 'I', 'think', 'that', 'is', 'a', 'term', 'but', 'I', 'dont', 'know', 'exactly', 'what', 'it', 'means', 'Thats', 'really', 'cool', 'I', 'need', 'to', 'go', '.', 'Thanks', 'for', 'chatting', '!', 'Hope', 'you', 'have', 'a', 'good', 'day', '!']\n"
     ]
    }
   ],
   "source": [
    "json_file_path = \"dataset.json\"\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    conversational_data = json.load(json_file)\n",
    "\n",
    "\n",
    "processed_data = preprocess_data(conversational_data)\n",
    "#print(\"processed_data\", processed_data)\n",
    "\n",
    "\n",
    "tokenized_data = [word_tokenize(text) for text in processed_data]\n",
    "\n",
    "\n",
    "\n",
    "print(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cae7b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "input_output_pairs = create_input_output_pairs(tokenized_data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ac19740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Hello', 'How', 'are', 'you', 'doing'], 'today'),\n",
       " (['How', 'are', 'you', 'doing', 'today'], '?'),\n",
       " (['are', 'you', 'doing', 'today', '?'], 'whats'),\n",
       " (['you', 'doing', 'today', '?', 'whats'], 'up'),\n",
       " (['doing', 'today', '?', 'whats', 'up'], 'MD'),\n",
       " (['today', '?', 'whats', 'up', 'MD'], 'im'),\n",
       " (['?', 'whats', 'up', 'MD', 'im'], 'doing'),\n",
       " (['whats', 'up', 'MD', 'im', 'doing'], 'good'),\n",
       " (['up', 'MD', 'im', 'doing', 'good'], 'how'),\n",
       " (['MD', 'im', 'doing', 'good', 'how'], 'are'),\n",
       " (['im', 'doing', 'good', 'how', 'are'], 'you'),\n",
       " (['doing', 'good', 'how', 'are', 'you'], 'doing'),\n",
       " (['good', 'how', 'are', 'you', 'doing'], '?'),\n",
       " (['how', 'are', 'you', 'doing', '?'], 'Im'),\n",
       " (['are', 'you', 'doing', '?', 'Im'], 'alright'),\n",
       " (['you', 'doing', '?', 'Im', 'alright'], ','),\n",
       " (['doing', '?', 'Im', 'alright', ','], 'I'),\n",
       " (['?', 'Im', 'alright', ',', 'I'], 'just'),\n",
       " (['Im', 'alright', ',', 'I', 'just'], 'took'),\n",
       " (['alright', ',', 'I', 'just', 'took'], 'a'),\n",
       " ([',', 'I', 'just', 'took', 'a'], 'nap'),\n",
       " (['I', 'just', 'took', 'a', 'nap'], '.'),\n",
       " (['just', 'took', 'a', 'nap', '.'], 'But'),\n",
       " (['took', 'a', 'nap', '.', 'But'], 'it'),\n",
       " (['a', 'nap', '.', 'But', 'it'], 'was'),\n",
       " (['nap', '.', 'But', 'it', 'was'], 'one'),\n",
       " (['.', 'But', 'it', 'was', 'one'], 'of'),\n",
       " (['But', 'it', 'was', 'one', 'of'], 'those'),\n",
       " (['it', 'was', 'one', 'of', 'those'], 'naps'),\n",
       " (['was', 'one', 'of', 'those', 'naps'], 'that'),\n",
       " (['one', 'of', 'those', 'naps', 'that'], 'doesnt'),\n",
       " (['of', 'those', 'naps', 'that', 'doesnt'], 'help'),\n",
       " (['those', 'naps', 'that', 'doesnt', 'help'], 'anything'),\n",
       " (['naps', 'that', 'doesnt', 'help', 'anything'], '.'),\n",
       " (['that', 'doesnt', 'help', 'anything', '.'], 'It'),\n",
       " (['doesnt', 'help', 'anything', '.', 'It'], 'just'),\n",
       " (['help', 'anything', '.', 'It', 'just'], 'makes'),\n",
       " (['anything', '.', 'It', 'just', 'makes'], 'everything'),\n",
       " (['.', 'It', 'just', 'makes', 'everything'], 'worse'),\n",
       " (['It', 'just', 'makes', 'everything', 'worse'], 'and'),\n",
       " (['just', 'makes', 'everything', 'worse', 'and'], 'you'),\n",
       " (['makes', 'everything', 'worse', 'and', 'you'], 'question'),\n",
       " (['everything', 'worse', 'and', 'you', 'question'], 'all'),\n",
       " (['worse', 'and', 'you', 'question', 'all'], 'your'),\n",
       " (['and', 'you', 'question', 'all', 'your'], 'life'),\n",
       " (['you', 'question', 'all', 'your', 'life'], 'choices'),\n",
       " (['question', 'all', 'your', 'life', 'choices'], 'oh'),\n",
       " (['all', 'your', 'life', 'choices', 'oh'], 'wow'),\n",
       " (['your', 'life', 'choices', 'oh', 'wow'], 'haha'),\n",
       " (['life', 'choices', 'oh', 'wow', 'haha'], 'so'),\n",
       " (['choices', 'oh', 'wow', 'haha', 'so'], 'you'),\n",
       " (['oh', 'wow', 'haha', 'so', 'you'], 'still'),\n",
       " (['wow', 'haha', 'so', 'you', 'still'], 'feel'),\n",
       " (['haha', 'so', 'you', 'still', 'feel'], 'tired'),\n",
       " (['so', 'you', 'still', 'feel', 'tired'], 'huh'),\n",
       " (['you', 'still', 'feel', 'tired', 'huh'], '?'),\n",
       " (['still', 'feel', 'tired', 'huh', '?'], 'Yeah'),\n",
       " (['feel', 'tired', 'huh', '?', 'Yeah'], 'did'),\n",
       " (['tired', 'huh', '?', 'Yeah', 'did'], 'you'),\n",
       " (['huh', '?', 'Yeah', 'did', 'you'], 'go'),\n",
       " (['?', 'Yeah', 'did', 'you', 'go'], 'to'),\n",
       " (['Yeah', 'did', 'you', 'go', 'to'], 'bed'),\n",
       " (['did', 'you', 'go', 'to', 'bed'], 'late'),\n",
       " (['you', 'go', 'to', 'bed', 'late'], '?'),\n",
       " (['go', 'to', 'bed', 'late', '?'], 'I'),\n",
       " (['to', 'bed', 'late', '?', 'I'], 'have'),\n",
       " (['bed', 'late', '?', 'I', 'have'], 'more'),\n",
       " (['late', '?', 'I', 'have', 'more'], 'of'),\n",
       " (['?', 'I', 'have', 'more', 'of'], 'a'),\n",
       " (['I', 'have', 'more', 'of', 'a'], 'head'),\n",
       " (['have', 'more', 'of', 'a', 'head'], 'ache'),\n",
       " (['more', 'of', 'a', 'head', 'ache'], 'than'),\n",
       " (['of', 'a', 'head', 'ache', 'than'], 'anything'),\n",
       " (['a', 'head', 'ache', 'than', 'anything'], 'else'),\n",
       " (['head', 'ache', 'than', 'anything', 'else'], 'dude'),\n",
       " (['ache', 'than', 'anything', 'else', 'dude'], 'thats'),\n",
       " (['than', 'anything', 'else', 'dude', 'thats'], 'terrible'),\n",
       " (['anything', 'else', 'dude', 'thats', 'terrible'], 'No'),\n",
       " (['else', 'dude', 'thats', 'terrible', 'No'], ','),\n",
       " (['dude', 'thats', 'terrible', 'No', ','], 'Its'),\n",
       " (['thats', 'terrible', 'No', ',', 'Its'], 'just'),\n",
       " (['terrible', 'No', ',', 'Its', 'just'], 'adjusting'),\n",
       " (['No', ',', 'Its', 'just', 'adjusting'], 'to'),\n",
       " ([',', 'Its', 'just', 'adjusting', 'to'], 'a'),\n",
       " (['Its', 'just', 'adjusting', 'to', 'a'], 'time'),\n",
       " (['just', 'adjusting', 'to', 'a', 'time'], 'change'),\n",
       " (['adjusting', 'to', 'a', 'time', 'change'], 'But'),\n",
       " (['to', 'a', 'time', 'change', 'But'], 'enough'),\n",
       " (['a', 'time', 'change', 'But', 'enough'], 'about'),\n",
       " (['time', 'change', 'But', 'enough', 'about'], 'me'),\n",
       " (['change', 'But', 'enough', 'about', 'me'], 'What'),\n",
       " (['But', 'enough', 'about', 'me', 'What'], 'are'),\n",
       " (['enough', 'about', 'me', 'What', 'are'], 'you'),\n",
       " (['about', 'me', 'What', 'are', 'you'], 'up'),\n",
       " (['me', 'What', 'are', 'you', 'up'], 'to'),\n",
       " (['What', 'are', 'you', 'up', 'to'], '?'),\n",
       " (['are', 'you', 'up', 'to', '?'], 'im'),\n",
       " (['you', 'up', 'to', '?', 'im'], 'refining'),\n",
       " (['up', 'to', '?', 'im', 'refining'], 'my'),\n",
       " (['to', '?', 'im', 'refining', 'my'], 'skills'),\n",
       " (['?', 'im', 'refining', 'my', 'skills'], 'on'),\n",
       " (['im', 'refining', 'my', 'skills', 'on'], 'Sketch'),\n",
       " (['refining', 'my', 'skills', 'on', 'Sketch'], 'App'),\n",
       " (['my', 'skills', 'on', 'Sketch', 'App'], 'have'),\n",
       " (['skills', 'on', 'Sketch', 'App', 'have'], 'you'),\n",
       " (['on', 'Sketch', 'App', 'have', 'you'], 'heard'),\n",
       " (['Sketch', 'App', 'have', 'you', 'heard'], 'of'),\n",
       " (['App', 'have', 'you', 'heard', 'of'], 'Sketch'),\n",
       " (['have', 'you', 'heard', 'of', 'Sketch'], '?'),\n",
       " (['you', 'heard', 'of', 'Sketch', '?'], 'I'),\n",
       " (['heard', 'of', 'Sketch', '?', 'I'], 'havent'),\n",
       " (['of', 'Sketch', '?', 'I', 'havent'], 'What'),\n",
       " (['Sketch', '?', 'I', 'havent', 'What'], 'is'),\n",
       " (['?', 'I', 'havent', 'What', 'is'], 'it'),\n",
       " (['I', 'havent', 'What', 'is', 'it'], '?'),\n",
       " (['havent', 'What', 'is', 'it', '?'], 'you'),\n",
       " (['What', 'is', 'it', '?', 'you'], 'know'),\n",
       " (['is', 'it', '?', 'you', 'know'], 'what'),\n",
       " (['it', '?', 'you', 'know', 'what'], 'adobe'),\n",
       " (['?', 'you', 'know', 'what', 'adobe'], 'illustrator'),\n",
       " (['you', 'know', 'what', 'adobe', 'illustrator'], 'is'),\n",
       " (['know', 'what', 'adobe', 'illustrator', 'is'], 'right'),\n",
       " (['what', 'adobe', 'illustrator', 'is', 'right'], '?'),\n",
       " (['adobe', 'illustrator', 'is', 'right', '?'], 'Yes'),\n",
       " (['illustrator', 'is', 'right', '?', 'Yes'], 'its'),\n",
       " (['is', 'right', '?', 'Yes', 'its'], 'a'),\n",
       " (['right', '?', 'Yes', 'its', 'a'], 'vector'),\n",
       " (['?', 'Yes', 'its', 'a', 'vector'], 'graphic'),\n",
       " (['Yes', 'its', 'a', 'vector', 'graphic'], 'editor'),\n",
       " (['its', 'a', 'vector', 'graphic', 'editor'], 'like'),\n",
       " (['a', 'vector', 'graphic', 'editor', 'like'], 'illustrator'),\n",
       " (['vector', 'graphic', 'editor', 'like', 'illustrator'], 'but'),\n",
       " (['graphic', 'editor', 'like', 'illustrator', 'but'], 'its'),\n",
       " (['editor', 'like', 'illustrator', 'but', 'its'], 'a'),\n",
       " (['like', 'illustrator', 'but', 'its', 'a'], 'lot'),\n",
       " (['illustrator', 'but', 'its', 'a', 'lot'], 'more'),\n",
       " (['but', 'its', 'a', 'lot', 'more'], 'light'),\n",
       " (['its', 'a', 'lot', 'more', 'light'], 'weight'),\n",
       " (['a', 'lot', 'more', 'light', 'weight'], 'and'),\n",
       " (['lot', 'more', 'light', 'weight', 'and'], 'its'),\n",
       " (['more', 'light', 'weight', 'and', 'its'], 'for'),\n",
       " (['light', 'weight', 'and', 'its', 'for'], 'UX'),\n",
       " (['weight', 'and', 'its', 'for', 'UX'], 'designers'),\n",
       " (['and', 'its', 'for', 'UX', 'designers'], 'to'),\n",
       " (['its', 'for', 'UX', 'designers', 'to'], 'design'),\n",
       " (['for', 'UX', 'designers', 'to', 'design'], 'UI'),\n",
       " (['UX', 'designers', 'to', 'design', 'UI'], 'Oh'),\n",
       " (['designers', 'to', 'design', 'UI', 'Oh'], 'nice'),\n",
       " (['to', 'design', 'UI', 'Oh', 'nice'], '!'),\n",
       " (['design', 'UI', 'Oh', 'nice', '!'], 'What'),\n",
       " (['UI', 'Oh', 'nice', '!', 'What'], 'have'),\n",
       " (['Oh', 'nice', '!', 'What', 'have'], 'you'),\n",
       " (['nice', '!', 'What', 'have', 'you'], 'been'),\n",
       " (['!', 'What', 'have', 'you', 'been'], 'practicing'),\n",
       " (['What', 'have', 'you', 'been', 'practicing'], 'designing'),\n",
       " (['have', 'you', 'been', 'practicing', 'designing'], 'Ive'),\n",
       " (['you', 'been', 'practicing', 'designing', 'Ive'], 'been'),\n",
       " (['been', 'practicing', 'designing', 'Ive', 'been'], 'trying'),\n",
       " (['practicing', 'designing', 'Ive', 'been', 'trying'], 'to'),\n",
       " (['designing', 'Ive', 'been', 'trying', 'to'], 'learn'),\n",
       " (['Ive', 'been', 'trying', 'to', 'learn'], 'more'),\n",
       " (['been', 'trying', 'to', 'learn', 'more'], 'tools'),\n",
       " (['trying', 'to', 'learn', 'more', 'tools'], 'on'),\n",
       " (['to', 'learn', 'more', 'tools', 'on'], 'sketch'),\n",
       " (['learn', 'more', 'tools', 'on', 'sketch'], 'and'),\n",
       " (['more', 'tools', 'on', 'sketch', 'and'], 'just'),\n",
       " (['tools', 'on', 'sketch', 'and', 'just'], 'get'),\n",
       " (['on', 'sketch', 'and', 'just', 'get'], 'faster'),\n",
       " (['sketch', 'and', 'just', 'get', 'faster'], 'at'),\n",
       " (['and', 'just', 'get', 'faster', 'at'], 'it'),\n",
       " (['just', 'get', 'faster', 'at', 'it'], ']'),\n",
       " (['get', 'faster', 'at', 'it', ']'], 'Im'),\n",
       " (['faster', 'at', 'it', ']', 'Im'], 'doing'),\n",
       " (['at', 'it', ']', 'Im', 'doing'], 'a'),\n",
       " (['it', ']', 'Im', 'doing', 'a'], 'free'),\n",
       " ([']', 'Im', 'doing', 'a', 'free'], 'lance'),\n",
       " (['Im', 'doing', 'a', 'free', 'lance'], 'job'),\n",
       " (['doing', 'a', 'free', 'lance', 'job'], 'for'),\n",
       " (['a', 'free', 'lance', 'job', 'for'], 'a'),\n",
       " (['free', 'lance', 'job', 'for', 'a'], 'professor'),\n",
       " (['lance', 'job', 'for', 'a', 'professor'], 'for'),\n",
       " (['job', 'for', 'a', 'professor', 'for'], 'his'),\n",
       " (['for', 'a', 'professor', 'for', 'his'], 'app'),\n",
       " (['a', 'professor', 'for', 'his', 'app'], 'idea'),\n",
       " (['professor', 'for', 'his', 'app', 'idea'], 'and'),\n",
       " (['for', 'his', 'app', 'idea', 'and'], 'he'),\n",
       " (['his', 'app', 'idea', 'and', 'he'], 'needs'),\n",
       " (['app', 'idea', 'and', 'he', 'needs'], 'a'),\n",
       " (['idea', 'and', 'he', 'needs', 'a'], 'mockup'),\n",
       " (['and', 'he', 'needs', 'a', 'mockup'], 'of'),\n",
       " (['he', 'needs', 'a', 'mockup', 'of'], 'what'),\n",
       " (['needs', 'a', 'mockup', 'of', 'what'], 'the'),\n",
       " (['a', 'mockup', 'of', 'what', 'the'], 'app'),\n",
       " (['mockup', 'of', 'what', 'the', 'app'], 'will'),\n",
       " (['of', 'what', 'the', 'app', 'will'], 'look'),\n",
       " (['what', 'the', 'app', 'will', 'look'], 'like'),\n",
       " (['the', 'app', 'will', 'look', 'like'], 'for'),\n",
       " (['app', 'will', 'look', 'like', 'for'], 'his'),\n",
       " (['will', 'look', 'like', 'for', 'his'], 'pitch'),\n",
       " (['look', 'like', 'for', 'his', 'pitch'], 'Wow'),\n",
       " (['like', 'for', 'his', 'pitch', 'Wow'], 'that'),\n",
       " (['for', 'his', 'pitch', 'Wow', 'that'], 'is'),\n",
       " (['his', 'pitch', 'Wow', 'that', 'is'], 'cool'),\n",
       " (['pitch', 'Wow', 'that', 'is', 'cool'], '!'),\n",
       " (['Wow', 'that', 'is', 'cool', '!'], 'When'),\n",
       " (['that', 'is', 'cool', '!', 'When'], 'is'),\n",
       " (['is', 'cool', '!', 'When', 'is'], 'that'),\n",
       " (['cool', '!', 'When', 'is', 'that'], 'all'),\n",
       " (['!', 'When', 'is', 'that', 'all'], 'due'),\n",
       " (['When', 'is', 'that', 'all', 'due'], 'by'),\n",
       " (['is', 'that', 'all', 'due', 'by'], '?'),\n",
       " (['that', 'all', 'due', 'by', '?'], 'this'),\n",
       " (['all', 'due', 'by', '?', 'this'], 'weekend'),\n",
       " (['due', 'by', '?', 'this', 'weekend'], 'haha'),\n",
       " (['by', '?', 'this', 'weekend', 'haha'], 'Do'),\n",
       " (['?', 'this', 'weekend', 'haha', 'Do'], 'you'),\n",
       " (['this', 'weekend', 'haha', 'Do', 'you'], 'think'),\n",
       " (['weekend', 'haha', 'Do', 'you', 'think'], 'you'),\n",
       " (['haha', 'Do', 'you', 'think', 'you'], 'will'),\n",
       " (['Do', 'you', 'think', 'you', 'will'], 'finish'),\n",
       " (['you', 'think', 'you', 'will', 'finish'], 'or'),\n",
       " (['think', 'you', 'will', 'finish', 'or'], 'is'),\n",
       " (['you', 'will', 'finish', 'or', 'is'], 'that'),\n",
       " (['will', 'finish', 'or', 'is', 'that'], 'a'),\n",
       " (['finish', 'or', 'is', 'that', 'a'], 'pushy'),\n",
       " (['or', 'is', 'that', 'a', 'pushy'], 'deadline'),\n",
       " (['is', 'that', 'a', 'pushy', 'deadline'], '?'),\n",
       " (['that', 'a', 'pushy', 'deadline', '?'], 'its'),\n",
       " (['a', 'pushy', 'deadline', '?', 'its'], 'a'),\n",
       " (['pushy', 'deadline', '?', 'its', 'a'], 'deadline'),\n",
       " (['deadline', '?', 'its', 'a', 'deadline'], 'I'),\n",
       " (['?', 'its', 'a', 'deadline', 'I'], 'set'),\n",
       " (['its', 'a', 'deadline', 'I', 'set'], 'haha'),\n",
       " (['a', 'deadline', 'I', 'set', 'haha'], 'but'),\n",
       " (['deadline', 'I', 'set', 'haha', 'but'], 'I'),\n",
       " (['I', 'set', 'haha', 'but', 'I'], 'think'),\n",
       " (['set', 'haha', 'but', 'I', 'think'], 'Ill'),\n",
       " (['haha', 'but', 'I', 'think', 'Ill'], 'be'),\n",
       " (['but', 'I', 'think', 'Ill', 'be'], 'able'),\n",
       " (['I', 'think', 'Ill', 'be', 'able'], 'to'),\n",
       " (['think', 'Ill', 'be', 'able', 'to'], 'finish'),\n",
       " (['Ill', 'be', 'able', 'to', 'finish'], 'I'),\n",
       " (['be', 'able', 'to', 'finish', 'I'], 'hope'),\n",
       " (['able', 'to', 'finish', 'I', 'hope'], '...'),\n",
       " (['to', 'finish', 'I', 'hope', '...'], 'if'),\n",
       " (['finish', 'I', 'hope', '...', 'if'], 'not'),\n",
       " (['I', 'hope', '...', 'if', 'not'], 'I'),\n",
       " (['hope', '...', 'if', 'not', 'I'], 'can'),\n",
       " (['...', 'if', 'not', 'I', 'can'], 'push'),\n",
       " (['if', 'not', 'I', 'can', 'push'], 'it'),\n",
       " (['not', 'I', 'can', 'push', 'it'], 'back'),\n",
       " (['I', 'can', 'push', 'it', 'back'], 'a'),\n",
       " (['can', 'push', 'it', 'back', 'a'], 'little'),\n",
       " (['push', 'it', 'back', 'a', 'little'], 'Well'),\n",
       " (['it', 'back', 'a', 'little', 'Well'], 'I'),\n",
       " (['back', 'a', 'little', 'Well', 'I'], 'wish'),\n",
       " (['a', 'little', 'Well', 'I', 'wish'], 'you'),\n",
       " (['little', 'Well', 'I', 'wish', 'you'], 'the'),\n",
       " (['Well', 'I', 'wish', 'you', 'the'], 'best'),\n",
       " (['I', 'wish', 'you', 'the', 'best'], 'of'),\n",
       " (['wish', 'you', 'the', 'best', 'of'], 'luck'),\n",
       " (['you', 'the', 'best', 'of', 'luck'], 'with'),\n",
       " (['the', 'best', 'of', 'luck', 'with'], 'that'),\n",
       " (['best', 'of', 'luck', 'with', 'that'], 'deadline'),\n",
       " (['of', 'luck', 'with', 'that', 'deadline'], 'thanks'),\n",
       " (['luck', 'with', 'that', 'deadline', 'thanks'], 'do'),\n",
       " (['with', 'that', 'deadline', 'thanks', 'do'], 'you'),\n",
       " (['that', 'deadline', 'thanks', 'do', 'you'], 'have'),\n",
       " (['deadline', 'thanks', 'do', 'you', 'have'], 'any'),\n",
       " (['thanks', 'do', 'you', 'have', 'any'], 'personal'),\n",
       " (['do', 'you', 'have', 'any', 'personal'], 'projects'),\n",
       " (['you', 'have', 'any', 'personal', 'projects'], 'you'),\n",
       " (['have', 'any', 'personal', 'projects', 'you'], 'are'),\n",
       " (['any', 'personal', 'projects', 'you', 'are'], 'working'),\n",
       " (['personal', 'projects', 'you', 'are', 'working'], 'on'),\n",
       " (['projects', 'you', 'are', 'working', 'on'], \"''\"),\n",
       " (['you', 'are', 'working', 'on', \"''\"], '?'),\n",
       " (['are', 'working', 'on', \"''\", '?'], 'Not'),\n",
       " (['working', 'on', \"''\", '?', 'Not'], 'really'),\n",
       " (['on', \"''\", '?', 'Not', 'really'], 'projects'),\n",
       " ([\"''\", '?', 'Not', 'really', 'projects'], '.'),\n",
       " (['?', 'Not', 'really', 'projects', '.'], 'I'),\n",
       " (['Not', 'really', 'projects', '.', 'I'], 'finished'),\n",
       " (['really', 'projects', '.', 'I', 'finished'], 'editing'),\n",
       " (['projects', '.', 'I', 'finished', 'editing'], 'a'),\n",
       " (['.', 'I', 'finished', 'editing', 'a'], 'bunch'),\n",
       " (['I', 'finished', 'editing', 'a', 'bunch'], 'of'),\n",
       " (['finished', 'editing', 'a', 'bunch', 'of'], 'engagement'),\n",
       " (['editing', 'a', 'bunch', 'of', 'engagement'], 'pictures'),\n",
       " (['a', 'bunch', 'of', 'engagement', 'pictures'], '.'),\n",
       " (['bunch', 'of', 'engagement', 'pictures', '.'], 'So'),\n",
       " (['of', 'engagement', 'pictures', '.', 'So'], 'now'),\n",
       " (['engagement', 'pictures', '.', 'So', 'now'], 'all'),\n",
       " (['pictures', '.', 'So', 'now', 'all'], 'I'),\n",
       " (['.', 'So', 'now', 'all', 'I'], 'am'),\n",
       " (['So', 'now', 'all', 'I', 'am'], 'really'),\n",
       " (['now', 'all', 'I', 'am', 'really'], 'doing'),\n",
       " (['all', 'I', 'am', 'really', 'doing'], 'is'),\n",
       " (['I', 'am', 'really', 'doing', 'is'], 'just'),\n",
       " (['am', 'really', 'doing', 'is', 'just'], 'practicing'),\n",
       " (['really', 'doing', 'is', 'just', 'practicing'], 'some'),\n",
       " (['doing', 'is', 'just', 'practicing', 'some'], 'sketching'),\n",
       " (['is', 'just', 'practicing', 'some', 'sketching'], 'I'),\n",
       " (['just', 'practicing', 'some', 'sketching', 'I'], 'can'),\n",
       " (['practicing', 'some', 'sketching', 'I', 'can'], 'always'),\n",
       " (['some', 'sketching', 'I', 'can', 'always'], 'practice'),\n",
       " (['sketching', 'I', 'can', 'always', 'practice'], 'and'),\n",
       " (['I', 'can', 'always', 'practice', 'and'], 'improve'),\n",
       " (['can', 'always', 'practice', 'and', 'improve'], 'that'),\n",
       " (['always', 'practice', 'and', 'improve', 'that'], 'skill'),\n",
       " (['practice', 'and', 'improve', 'that', 'skill'], 'oh'),\n",
       " (['and', 'improve', 'that', 'skill', 'oh'], 'coooool'),\n",
       " (['improve', 'that', 'skill', 'oh', 'coooool'], '!'),\n",
       " (['that', 'skill', 'oh', 'coooool', '!'], 'was'),\n",
       " (['skill', 'oh', 'coooool', '!', 'was'], 'this'),\n",
       " (['oh', 'coooool', '!', 'was', 'this'], 'a'),\n",
       " (['coooool', '!', 'was', 'this', 'a'], 'paid'),\n",
       " (['!', 'was', 'this', 'a', 'paid'], 'photoshoot'),\n",
       " (['was', 'this', 'a', 'paid', 'photoshoot'], '?'),\n",
       " (['this', 'a', 'paid', 'photoshoot', '?'], 'Yep'),\n",
       " (['a', 'paid', 'photoshoot', '?', 'Yep'], ','),\n",
       " (['paid', 'photoshoot', '?', 'Yep', ','], 'I'),\n",
       " (['photoshoot', '?', 'Yep', ',', 'I'], 'had'),\n",
       " (['?', 'Yep', ',', 'I', 'had'], 'a'),\n",
       " (['Yep', ',', 'I', 'had', 'a'], 'few'),\n",
       " ([',', 'I', 'had', 'a', 'few'], 'that'),\n",
       " (['I', 'had', 'a', 'few', 'that'], 'I'),\n",
       " (['had', 'a', 'few', 'that', 'I'], 'did'),\n",
       " (['a', 'few', 'that', 'I', 'did'], 'before'),\n",
       " (['few', 'that', 'I', 'did', 'before'], 'I'),\n",
       " (['that', 'I', 'did', 'before', 'I'], 'went'),\n",
       " (['I', 'did', 'before', 'I', 'went'], 'home'),\n",
       " (['did', 'before', 'I', 'went', 'home'], 'for'),\n",
       " (['before', 'I', 'went', 'home', 'for'], 'the'),\n",
       " (['I', 'went', 'home', 'for', 'the'], 'summer'),\n",
       " (['went', 'home', 'for', 'the', 'summer'], 'engagement'),\n",
       " (['home', 'for', 'the', 'summer', 'engagement'], 'shoots'),\n",
       " (['for', 'the', 'summer', 'engagement', 'shoots'], 'or'),\n",
       " (['the', 'summer', 'engagement', 'shoots', 'or'], 'weddings'),\n",
       " (['summer', 'engagement', 'shoots', 'or', 'weddings'], 'are'),\n",
       " (['engagement', 'shoots', 'or', 'weddings', 'are'], 'goooood'),\n",
       " (['shoots', 'or', 'weddings', 'are', 'goooood'], 'money'),\n",
       " (['or', 'weddings', 'are', 'goooood', 'money'], 'but'),\n",
       " (['weddings', 'are', 'goooood', 'money', 'but'], 'i'),\n",
       " (['are', 'goooood', 'money', 'but', 'i'], 'hear'),\n",
       " (['goooood', 'money', 'but', 'i', 'hear'], 'in'),\n",
       " (['money', 'but', 'i', 'hear', 'in'], 'utah'),\n",
       " (['but', 'i', 'hear', 'in', 'utah'], 'its'),\n",
       " (['i', 'hear', 'in', 'utah', 'its'], 'super'),\n",
       " (['hear', 'in', 'utah', 'its', 'super'], 'competitive'),\n",
       " (['in', 'utah', 'its', 'super', 'competitive'], 'Yeah'),\n",
       " (['utah', 'its', 'super', 'competitive', 'Yeah'], 'there'),\n",
       " (['its', 'super', 'competitive', 'Yeah', 'there'], 'are'),\n",
       " (['super', 'competitive', 'Yeah', 'there', 'are'], 'so'),\n",
       " (['competitive', 'Yeah', 'there', 'are', 'so'], 'many'),\n",
       " (['Yeah', 'there', 'are', 'so', 'many'], ','),\n",
       " (['there', 'are', 'so', 'many', ','], 'every'),\n",
       " (['are', 'so', 'many', ',', 'every'], 'other'),\n",
       " (['so', 'many', ',', 'every', 'other'], 'person'),\n",
       " (['many', ',', 'every', 'other', 'person'], 'takes'),\n",
       " ([',', 'every', 'other', 'person', 'takes'], 'pictures'),\n",
       " (['every', 'other', 'person', 'takes', 'pictures'], 'Its'),\n",
       " (['other', 'person', 'takes', 'pictures', 'Its'], 'easy'),\n",
       " (['person', 'takes', 'pictures', 'Its', 'easy'], 'to'),\n",
       " (['takes', 'pictures', 'Its', 'easy', 'to'], 'do'),\n",
       " (['pictures', 'Its', 'easy', 'to', 'do'], 'it'),\n",
       " (['Its', 'easy', 'to', 'do', 'it'], 'for'),\n",
       " (['easy', 'to', 'do', 'it', 'for'], 'friends'),\n",
       " (['to', 'do', 'it', 'for', 'friends'], 'and'),\n",
       " (['do', 'it', 'for', 'friends', 'and'], 'people'),\n",
       " (['it', 'for', 'friends', 'and', 'people'], 'you'),\n",
       " (['for', 'friends', 'and', 'people', 'you'], 'know'),\n",
       " (['friends', 'and', 'people', 'you', 'know'], 'but'),\n",
       " (['and', 'people', 'you', 'know', 'but'], 'past'),\n",
       " (['people', 'you', 'know', 'but', 'past'], 'that'),\n",
       " (['you', 'know', 'but', 'past', 'that'], 'it'),\n",
       " (['know', 'but', 'past', 'that', 'it'], 'is'),\n",
       " (['but', 'past', 'that', 'it', 'is'], 'hard'),\n",
       " (['past', 'that', 'it', 'is', 'hard'], 'to'),\n",
       " (['that', 'it', 'is', 'hard', 'to'], 'get'),\n",
       " (['it', 'is', 'hard', 'to', 'get'], 'more'),\n",
       " (['is', 'hard', 'to', 'get', 'more'], 'business'),\n",
       " (['hard', 'to', 'get', 'more', 'business'], 'yeah'),\n",
       " (['to', 'get', 'more', 'business', 'yeah'], 'true'),\n",
       " (['get', 'more', 'business', 'yeah', 'true'], 'do'),\n",
       " (['more', 'business', 'yeah', 'true', 'do'], 'you'),\n",
       " (['business', 'yeah', 'true', 'do', 'you'], 'think'),\n",
       " (['yeah', 'true', 'do', 'you', 'think'], 'if'),\n",
       " (['true', 'do', 'you', 'think', 'if'], 'you'),\n",
       " (['do', 'you', 'think', 'if', 'you'], 'wanted'),\n",
       " (['you', 'think', 'if', 'you', 'wanted'], 'to'),\n",
       " (['think', 'if', 'you', 'wanted', 'to'], 'pursue'),\n",
       " (['if', 'you', 'wanted', 'to', 'pursue'], 'your'),\n",
       " (['you', 'wanted', 'to', 'pursue', 'your'], 'photography'),\n",
       " (['wanted', 'to', 'pursue', 'your', 'photography'], 'you'),\n",
       " (['to', 'pursue', 'your', 'photography', 'you'], 'wuold'),\n",
       " (['pursue', 'your', 'photography', 'you', 'wuold'], 'have'),\n",
       " (['your', 'photography', 'you', 'wuold', 'have'], 'to'),\n",
       " (['photography', 'you', 'wuold', 'have', 'to'], 'move'),\n",
       " (['you', 'wuold', 'have', 'to', 'move'], 'out'),\n",
       " (['wuold', 'have', 'to', 'move', 'out'], 'of'),\n",
       " (['have', 'to', 'move', 'out', 'of'], 'utah'),\n",
       " (['to', 'move', 'out', 'of', 'utah'], '?'),\n",
       " (['move', 'out', 'of', 'utah', '?'], 'Um'),\n",
       " (['out', 'of', 'utah', '?', 'Um'], 'maybe'),\n",
       " (['of', 'utah', '?', 'Um', 'maybe'], ','),\n",
       " (['utah', '?', 'Um', 'maybe', ','], 'People'),\n",
       " (['?', 'Um', 'maybe', ',', 'People'], 'need'),\n",
       " (['Um', 'maybe', ',', 'People', 'need'], 'pictures'),\n",
       " (['maybe', ',', 'People', 'need', 'pictures'], 'everywhere'),\n",
       " ([',', 'People', 'need', 'pictures', 'everywhere'], 'But'),\n",
       " (['People', 'need', 'pictures', 'everywhere', 'But'], 'I'),\n",
       " (['need', 'pictures', 'everywhere', 'But', 'I'], 'dont'),\n",
       " (['pictures', 'everywhere', 'But', 'I', 'dont'], 'really'),\n",
       " (['everywhere', 'But', 'I', 'dont', 'really'], 'want'),\n",
       " (['But', 'I', 'dont', 'really', 'want'], 'to'),\n",
       " (['I', 'dont', 'really', 'want', 'to'], 'do'),\n",
       " (['dont', 'really', 'want', 'to', 'do'], 'that'),\n",
       " (['really', 'want', 'to', 'do', 'that'], ','),\n",
       " (['want', 'to', 'do', 'that', ','], 'its'),\n",
       " (['to', 'do', 'that', ',', 'its'], 'fine'),\n",
       " (['do', 'that', ',', 'its', 'fine'], 'for'),\n",
       " (['that', ',', 'its', 'fine', 'for'], 'the'),\n",
       " ([',', 'its', 'fine', 'for', 'the'], 'side'),\n",
       " (['its', 'fine', 'for', 'the', 'side'], ','),\n",
       " (['fine', 'for', 'the', 'side', ','], 'but'),\n",
       " (['for', 'the', 'side', ',', 'but'], 'too'),\n",
       " (['the', 'side', ',', 'but', 'too'], 'hard'),\n",
       " (['side', ',', 'but', 'too', 'hard'], 'to'),\n",
       " ([',', 'but', 'too', 'hard', 'to'], 'make'),\n",
       " (['but', 'too', 'hard', 'to', 'make'], 'a'),\n",
       " (['too', 'hard', 'to', 'make', 'a'], 'living'),\n",
       " (['hard', 'to', 'make', 'a', 'living'], 'on'),\n",
       " (['to', 'make', 'a', 'living', 'on'], 'Remind'),\n",
       " (['make', 'a', 'living', 'on', 'Remind'], 'me'),\n",
       " (['a', 'living', 'on', 'Remind', 'me'], 'what'),\n",
       " (['living', 'on', 'Remind', 'me', 'what'], 'you'),\n",
       " (['on', 'Remind', 'me', 'what', 'you'], 'want'),\n",
       " (['Remind', 'me', 'what', 'you', 'want'], 'to'),\n",
       " (['me', 'what', 'you', 'want', 'to'], 'do'),\n",
       " (['what', 'you', 'want', 'to', 'do'], 'career'),\n",
       " (['you', 'want', 'to', 'do', 'career'], 'wise'),\n",
       " (['want', 'to', 'do', 'career', 'wise'], '?'),\n",
       " (['to', 'do', 'career', 'wise', '?'], 'I'),\n",
       " (['do', 'career', 'wise', '?', 'I'], 'want'),\n",
       " (['career', 'wise', '?', 'I', 'want'], 'to'),\n",
       " (['wise', '?', 'I', 'want', 'to'], 'become'),\n",
       " (['?', 'I', 'want', 'to', 'become'], 'a'),\n",
       " (['I', 'want', 'to', 'become', 'a'], 'UX'),\n",
       " (['want', 'to', 'become', 'a', 'UX'], 'desiner'),\n",
       " (['to', 'become', 'a', 'UX', 'desiner'], 'if'),\n",
       " (['become', 'a', 'UX', 'desiner', 'if'], 'you'),\n",
       " (['a', 'UX', 'desiner', 'if', 'you'], 'werent'),\n",
       " (['UX', 'desiner', 'if', 'you', 'werent'], 'Christian'),\n",
       " (['desiner', 'if', 'you', 'werent', 'Christian'], 'what'),\n",
       " (['if', 'you', 'werent', 'Christian', 'what'], 'religion'),\n",
       " (['you', 'werent', 'Christian', 'what', 'religion'], 'would'),\n",
       " (['werent', 'Christian', 'what', 'religion', 'would'], 'you'),\n",
       " (['Christian', 'what', 'religion', 'would', 'you'], 'be'),\n",
       " (['what', 'religion', 'would', 'you', 'be'], 'in'),\n",
       " (['religion', 'would', 'you', 'be', 'in'], 'if'),\n",
       " (['would', 'you', 'be', 'in', 'if'], 'at'),\n",
       " (['you', 'be', 'in', 'if', 'at'], 'all'),\n",
       " (['be', 'in', 'if', 'at', 'all'], '?'),\n",
       " (['in', 'if', 'at', 'all', '?'], 'Hm'),\n",
       " (['if', 'at', 'all', '?', 'Hm'], 'Maybe'),\n",
       " (['at', 'all', '?', 'Hm', 'Maybe'], 'Buddhist'),\n",
       " (['all', '?', 'Hm', 'Maybe', 'Buddhist'], 'What'),\n",
       " (['?', 'Hm', 'Maybe', 'Buddhist', 'What'], 'about'),\n",
       " (['Hm', 'Maybe', 'Buddhist', 'What', 'about'], 'you'),\n",
       " (['Maybe', 'Buddhist', 'What', 'about', 'you'], '?'),\n",
       " (['Buddhist', 'What', 'about', 'you', '?'], 'I'),\n",
       " (['What', 'about', 'you', '?', 'I'], 'think'),\n",
       " (['about', 'you', '?', 'I', 'think'], 'its'),\n",
       " (['you', '?', 'I', 'think', 'its'], 'a'),\n",
       " (['?', 'I', 'think', 'its', 'a'], 'really'),\n",
       " (['I', 'think', 'its', 'a', 'really'], 'peaceful'),\n",
       " (['think', 'its', 'a', 'really', 'peaceful'], 'practice'),\n",
       " (['its', 'a', 'really', 'peaceful', 'practice'], 'and'),\n",
       " (['a', 'really', 'peaceful', 'practice', 'and'], 'it'),\n",
       " (['really', 'peaceful', 'practice', 'and', 'it'], 'would'),\n",
       " (['peaceful', 'practice', 'and', 'it', 'would'], 'match'),\n",
       " (['practice', 'and', 'it', 'would', 'match'], 'my'),\n",
       " (['and', 'it', 'would', 'match', 'my'], 'personality'),\n",
       " (['it', 'would', 'match', 'my', 'personality'], 'I'),\n",
       " (['would', 'match', 'my', 'personality', 'I'], 'probably'),\n",
       " (['match', 'my', 'personality', 'I', 'probably'], 'would'),\n",
       " (['my', 'personality', 'I', 'probably', 'would'], 'be'),\n",
       " (['personality', 'I', 'probably', 'would', 'be'], 'buddhist'),\n",
       " (['I', 'probably', 'would', 'be', 'buddhist'], 'culturally'),\n",
       " (['probably', 'would', 'be', 'buddhist', 'culturally'], 'because'),\n",
       " (['would', 'be', 'buddhist', 'culturally', 'because'], 'IM'),\n",
       " (['be', 'buddhist', 'culturally', 'because', 'IM'], 'korean'),\n",
       " (['buddhist', 'culturally', 'because', 'IM', 'korean'], 'but'),\n",
       " (['culturally', 'because', 'IM', 'korean', 'but'], 'if'),\n",
       " (['because', 'IM', 'korean', 'but', 'if'], 'i'),\n",
       " (['IM', 'korean', 'but', 'if', 'i'], 'had'),\n",
       " (['korean', 'but', 'if', 'i', 'had'], 'a'),\n",
       " (['but', 'if', 'i', 'had', 'a'], 'choice'),\n",
       " (['if', 'i', 'had', 'a', 'choice'], 'I'),\n",
       " (['i', 'had', 'a', 'choice', 'I'], 'would'),\n",
       " (['had', 'a', 'choice', 'I', 'would'], \"n't\"),\n",
       " (['a', 'choice', 'I', 'would', \"n't\"], 'label'),\n",
       " (['choice', 'I', 'would', \"n't\", 'label'], 'myself'),\n",
       " (['I', 'would', \"n't\", 'label', 'myself'], 'as'),\n",
       " (['would', \"n't\", 'label', 'myself', 'as'], 'any'),\n",
       " ([\"n't\", 'label', 'myself', 'as', 'any'], 'religion'),\n",
       " (['label', 'myself', 'as', 'any', 'religion'], 'egnostic'),\n",
       " (['myself', 'as', 'any', 'religion', 'egnostic'], '?'),\n",
       " (['as', 'any', 'religion', 'egnostic', '?'], 'is'),\n",
       " (['any', 'religion', 'egnostic', '?', 'is'], 'that'),\n",
       " (['religion', 'egnostic', '?', 'is', 'that'], 'the'),\n",
       " (['egnostic', '?', 'is', 'that', 'the'], 'term'),\n",
       " (['?', 'is', 'that', 'the', 'term'], '?'),\n",
       " (['is', 'that', 'the', 'term', '?'], 'I'),\n",
       " (['that', 'the', 'term', '?', 'I'], 'think'),\n",
       " (['the', 'term', '?', 'I', 'think'], 'that'),\n",
       " (['term', '?', 'I', 'think', 'that'], 'is'),\n",
       " (['?', 'I', 'think', 'that', 'is'], 'a'),\n",
       " (['I', 'think', 'that', 'is', 'a'], 'term'),\n",
       " (['think', 'that', 'is', 'a', 'term'], 'but'),\n",
       " (['that', 'is', 'a', 'term', 'but'], 'I'),\n",
       " (['is', 'a', 'term', 'but', 'I'], 'dont'),\n",
       " (['a', 'term', 'but', 'I', 'dont'], 'know'),\n",
       " (['term', 'but', 'I', 'dont', 'know'], 'exactly'),\n",
       " (['but', 'I', 'dont', 'know', 'exactly'], 'what'),\n",
       " (['I', 'dont', 'know', 'exactly', 'what'], 'it'),\n",
       " (['dont', 'know', 'exactly', 'what', 'it'], 'means'),\n",
       " (['know', 'exactly', 'what', 'it', 'means'], 'Thats'),\n",
       " (['exactly', 'what', 'it', 'means', 'Thats'], 'really'),\n",
       " (['what', 'it', 'means', 'Thats', 'really'], 'cool'),\n",
       " (['it', 'means', 'Thats', 'really', 'cool'], 'I'),\n",
       " (['means', 'Thats', 'really', 'cool', 'I'], 'need'),\n",
       " (['Thats', 'really', 'cool', 'I', 'need'], 'to'),\n",
       " (['really', 'cool', 'I', 'need', 'to'], 'go'),\n",
       " (['cool', 'I', 'need', 'to', 'go'], '.'),\n",
       " (['I', 'need', 'to', 'go', '.'], 'Thanks'),\n",
       " (['need', 'to', 'go', '.', 'Thanks'], 'for'),\n",
       " (['to', 'go', '.', 'Thanks', 'for'], 'chatting'),\n",
       " (['go', '.', 'Thanks', 'for', 'chatting'], '!'),\n",
       " (['.', 'Thanks', 'for', 'chatting', '!'], 'Hope'),\n",
       " (['Thanks', 'for', 'chatting', '!', 'Hope'], 'you'),\n",
       " (['for', 'chatting', '!', 'Hope', 'you'], 'have'),\n",
       " (['chatting', '!', 'Hope', 'you', 'have'], 'a'),\n",
       " (['!', 'Hope', 'you', 'have', 'a'], 'good'),\n",
       " (['Hope', 'you', 'have', 'a', 'good'], 'day'),\n",
       " (['you', 'have', 'a', 'good', 'day'], '!')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_output_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82379c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634356e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.5.0\n",
      "  Using cached transformers-4.5.0-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from transformers==4.5.0) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from transformers==4.5.0) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from transformers==4.5.0) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from transformers==4.5.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from transformers==4.5.0) (2.31.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from transformers==4.5.0) (0.0.43)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.5.0)\n",
      "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from transformers==4.5.0) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.5.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from requests->transformers==4.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from requests->transformers==4.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from requests->transformers==4.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from requests->transformers==4.5.0) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.5.0) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.5.0) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.5.0) (1.2.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [51 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.5.0\n",
    "#!pip install transformers==4.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76adb2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5ab0a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/d6/a8/43e5033f9b2f727c158456e0720f870030ad3685c46f41ca3ca901b54922/torch-2.1.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from torch) (2023.12.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dashu\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.1.1-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/192.3 MB 3.6 MB/s eta 0:00:53\n",
      "   ---------------------------------------- 0.4/192.3 MB 5.3 MB/s eta 0:00:37\n",
      "   ---------------------------------------- 0.7/192.3 MB 6.2 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 1.3/192.3 MB 8.3 MB/s eta 0:00:23\n",
      "    --------------------------------------- 2.6/192.3 MB 11.8 MB/s eta 0:00:17\n",
      "    --------------------------------------- 4.7/192.3 MB 18.8 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 7.0/192.3 MB 23.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 9.9/192.3 MB 27.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 11.3/192.3 MB 46.7 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 14.0/192.3 MB 46.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 17.1/192.3 MB 50.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 20.1/192.3 MB 54.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 21.3/192.3 MB 46.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 22.8/192.3 MB 50.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 24.9/192.3 MB 46.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 27.1/192.3 MB 43.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 28.7/192.3 MB 38.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 30.8/192.3 MB 36.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 33.1/192.3 MB 43.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 35.2/192.3 MB 43.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 37.4/192.3 MB 43.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 39.5/192.3 MB 46.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 41.3/192.3 MB 43.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 43.2/192.3 MB 43.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 45.0/192.3 MB 40.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 47.1/192.3 MB 43.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 49.0/192.3 MB 38.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 51.0/192.3 MB 40.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 53.2/192.3 MB 40.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 54.9/192.3 MB 43.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 57.4/192.3 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 58.8/192.3 MB 43.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 61.3/192.3 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 63.1/192.3 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 65.1/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 67.5/192.3 MB 43.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 69.5/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 71.9/192.3 MB 46.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 73.3/192.3 MB 46.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 75.7/192.3 MB 46.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 77.7/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 80.5/192.3 MB 46.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 82.4/192.3 MB 46.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 83.8/192.3 MB 46.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 86.5/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 88.6/192.3 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 90.0/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 92.4/192.3 MB 43.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 94.6/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 96.0/192.3 MB 43.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 98.3/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 100.6/192.3 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 102.7/192.3 MB 46.7 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 105.2/192.3 MB 46.7 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 107.4/192.3 MB 50.4 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 110.4/192.3 MB 50.4 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 113.3/192.3 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 116.5/192.3 MB 73.1 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 117.8/192.3 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 119.6/192.3 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 122.3/192.3 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 126.4/192.3 MB 54.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 128.6/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 130.0/192.3 MB 50.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 133.2/192.3 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 136.0/192.3 MB 50.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 139.1/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 143.0/192.3 MB 73.1 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 145.6/192.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 148.9/192.3 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 152.7/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 155.6/192.3 MB 72.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 158.1/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 160.9/192.3 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 163.7/192.3 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 166.9/192.3 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 170.6/192.3 MB 65.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 173.7/192.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 177.0/192.3 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 180.0/192.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 183.6/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 186.6/192.3 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  189.8/192.3 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 192.3/192.3 MB 14.5 MB/s eta 0:00:00\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55676cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair (['you', 'have', 'a', 'good', 'day'], '!')\n",
      "input_sequence ['you', 'have', 'a', 'good', 'day']\n",
      "output_sequence !\n",
      "input_text you have a good day\n",
      "output_text !\n",
      "input_ids [5832, 14150, 64, 11274, 820]\n",
      "output_ids [0]\n",
      "pair (['but', 'its', 'a', 'lot', 'more'], 'light')\n",
      "input_sequence ['but', 'its', 'a', 'lot', 'more']\n",
      "output_sequence light\n",
      "input_text but its a lot more\n",
      "output_text light\n",
      "input_ids [4360, 896, 64, 26487, 3549]\n",
      "output_ids [2971]\n",
      "pair (['weddings', 'are', 'goooood', 'money', 'but'], 'i')\n",
      "input_sequence ['weddings', 'are', 'goooood', 'money', 'but']\n",
      "output_sequence i\n",
      "input_text weddings are goooood money but\n",
      "output_text i\n",
      "input_ids [50256, 533, 50256, 26316, 4360]\n",
      "output_ids [72]\n",
      "uuuu\n",
      "kkk\n",
      "llll\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllll\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1075\u001b[0m     input_ids,\n\u001b[0;32m   1076\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1077\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1078\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1079\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1080\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1081\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1082\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1083\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1084\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1085\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1086\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1087\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:776\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m--> 776\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    777\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    778\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, input_output_pairs, tokenizer):\n",
    "        self.input_output_pairs = input_output_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_output_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.input_output_pairs[idx]\n",
    "        print(\"pair\", pair)\n",
    "        \n",
    "        input_sequence = pair[0]\n",
    "        print(\"input_sequence\", input_sequence)\n",
    "        output_sequence = pair[1]\n",
    "        print(\"output_sequence\", output_sequence)\n",
    "\n",
    "        \n",
    "        input_text = \" \".join(input_sequence)  \n",
    "        print(\"input_text\", input_text)\n",
    "        output_text = output_sequence\n",
    "        print(\"output_text\", output_text)\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_sequence, add_special_tokens=True)\n",
    "        print(\"input_ids\", input_ids)\n",
    "        output_ids = self.tokenizer.encode(output_sequence, add_special_tokens=True)\n",
    "        print(\"output_ids\", output_ids)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": output_ids}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "dataset = ConversationDataset(input_output_pairs, tokenizer)\n",
    "#print(dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "#print(\"dataloader\",dataloader)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        print(\"uuuu\")\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        print(\"kkk\")\n",
    "        labels = batch[\"labels\"]\n",
    "        print(\"llll\")\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        print(\"mmm\")\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679ad8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "502627f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f2f3faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d1706bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7911633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce8510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55a421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
